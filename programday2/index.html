<!DOCTYPE html>
<html lang='en'>

<head>
    <base href="..">
    <link rel="shortcut icon" type="image/png" href="assets/favicon.png"/>
    <link rel="stylesheet" type="text/css" media="all" href="assets/main.css"/>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=default">
    </script>
    <meta name="description" content="Conference Template">
    <meta name="resource-type" content="document">
    <meta name="distribution" content="global">
    <meta name="KeyWords" content="Conference">
    <title>Program | Conference Template</title>
</head>

<body>

    <div class="banner">
        <img src="assets/banner.jpeg" alt="Conference Template Banner">
        <div class="top-left">
            <span class="title2">Workshop on</span> <span class="title1">Collaborative Learning</span> 
            <!-- <br><br> <br><br> <span class="title1">MBZUAI</span> -->
            <br><br>
            <span class="year">From Theory to Practice</span>
        </div>
        <div class="bottom-right">
            October 8-9, 2022 <br> MBZUAI, Abu Dhabi
        </div>
    </div>

    <table class="navigation">
        <tr>
            <td class="navigation">
                <a class="current" title="Conference Home Page" href=".">Home</a>
            </td>
            <td class="navigation">
                <a title="Conference Program Day 1" href="programday1">Program Day 1</a> 
            </td>
            <td class="navigation">
                <a title="Conference Program Day 2" href="programday2">Program Day 2</a> 
            </td>
        </tr>
    </table>


    <h2>Day 2 Program (Oct 9, Sun)</h2>

    <table>
        <tr>
            <td class="date" rowspan="2">
                9:00am
            </td>
            <td class="title-special">
                Breakfast!
            </td>
        </tr>
        <tr>
            <td class="abstract">
                <!-- Spacing -->
            </td>
        </tr>
    </table>

    <table id="Salman Avestimehr">
        <tr>
            <td class="date" rowspan="3">
                9:30am
            </td>
            <td class="title">
                Three daunting challenges of federated learning: privacy leakage, label deficiency, and resource constraints
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://www.avestimehr.com/"><b>Salman Avestimehr</b></a> (USC and FedML)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                Federated learning (FL) has emerged as a promising approach to to enable decentralized machine learning directly at the edge, in order to enhance users&apos; privacy, comply with regulations, and reduce development costs. In this talk, I will provide an overview of FL and highlight three fundamental challenges for landing FL into practice: (1) privacy and security guarantees for FL; (2) label scarcity at the edge; and (3) FL over resource-constrained edge nodes. I will also provide a brief overview of FedML (https://fedml.ai), which is a platform that enables zero-code, lightweight, cross-platform, and provably secure federated learning and analytics.
            </td>
        </tr>
    </table>
    <table id="Praneeth Vepakomma">
        <tr>
            <td class="date" rowspan="3">
                10:30am
            </td>
            <td class="title">
                A pan-disciplinary view of distributed & private computation: Statistics, Geometry, ML & Social Choice
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="http://praneeth.mit.edu/"><b>Praneeth Vepakomma</b></a> (MIT)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                Data in today&apos;s world is increasingly siloed across a wide variety of entities with varying resource constraints in collaboratively processing such data in order to draw actionable insights and wisdom. The quality of such wisdom is substantially better if such data is centralized at one spot prior to processing it, but this is prohibited due to privacy regulations, computational constraints, communication constraints, trade-secrets, trust issues and competition. This necessitates the development of distributed algorithms that are resource efficient for the entities involved while also preserving the privacy of this data and still ensure that the quality of wisdom obtained is on par with the case of data centralization. This talk covers some novel methods for the same in a pan-disciplinary manner tackling upstream problems with view-points in statistics, geometry, machine learning and social choice. Upstream problems are those root problems which when solved result in feeding into the solutions for several downstream problems leading to multi-pronged downstream impact.
            </td>
        </tr>
    </table>
    <table class="plenary">
        <tr>
            <td class="date" rowspan="3">
                10:00am
            </td>
            <td class="title">
                TBA
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="http://www.cs.cmu.edu/~epxing/"><b>Eric Xing</b></a> (Mohamed Bin Zayed University of Artificial Intelligence)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                TBA
            </td>
        </tr>
    </table>
    <table>
        <tr>
            <td class="date" rowspan="2">
                11:00am
            </td>
            <td class="title-special">
                Coffee &amp; Tea Break
            </td>
        </tr>
        <tr>
            <td class="abstract">
                <!-- Kept for Spacing -->
            </td>
        </tr>
    </table>
    <table>
        <tr>
            <td class="date" rowspan="3">
                11:30am
            </td>
            <td class="title">
                The Fed-BioMed Project: Deploying Federated Learning to Real-World Healthcare Applications
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://marcolorenzi.github.io/"><b>Marco Lorenzi</b></a> (Inria Sophia Antipolis)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                Fed-BioMed is a research and development initiative aiming at translating federated learning (FL) to healthcare applications. The deployment of federated learning requires to tackle important challenges to meet the strict requirements of real-world conditions. While typical methodological issues concern the definition of optimization frameworks to handle clients&apos; heterogeneity and guarantee unbiasedness, the translation of federated learning research into practice poses novel technical and societal questions. Typical problems to be addressed concern FL security, scalability and interoperability, which motivate novel research directions and promote the close interactions between researchers, technicians and healthcare practitioners.<br>
                During the talk I will provide an illustrations of the interplay between methodological development and translational effort that characterise the development of the Fed-BioMed FL platform, and discuss our current effort in delivering FL in hospitals networks.
            </td>
        </tr>
    </table>
    <table>
        <tr>
            <td class="date" rowspan="3">
                12:00pm
            </td>
            <td class="title">
                Secure and Federated Algorithms for Collaborative Genomic Studies
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://hhcho.com/"><b>Hoon Cho</b></a> (Broad Institute of MIT and Harvard)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                Genomic data are commonly held in data silos due to their sensitivity. This presents a key hurdle in genomics research, where access to data from large and diverse cohorts is crucial for extracting accurate biomedical insights. In this talk, I will describe our recent progress in facilitating cross-institutional collaboration in genomics with novel algorithmic tools. Synthesizing a range of modern techniques from applied cryptography, distributed algorithms and statistical genetics, we develop practical, secure and federated protocols for essential analysis tasks in genomics. These include the two most widely-used methods for genome-wide association studies (GWAS) based on principal component analysis (PCA) and linear mixed models (LMM). We demonstrate our methods on biobank-scale genomic datasets including hundreds of thousands of genomes. Finally, I will describe our recent efforts to deploy our tools to conduct a joint study between two large-scale genomic data repositories in the US that have never been jointly analyzed due to data sharing restrictions. Our work lays the foundation for broader collaboration in biomedical research.
            </td>
        </tr>
    </table>
    <table>
        <tr>
            <td class="date" rowspan="2">
                12:30am
            </td>
            <td class="title-special">
                Lunch Break
            </td>
        </tr>
        <tr>
            <td class="abstract">
                <!-- Kept for Spacing -->
            </td>
        </tr>
    </table>

    <table id="Anwar Hithnawi">
        <tr>
            <td class="date" rowspan="3">
                1:00pm
            </td>
            <td class="title">
                Security and Robustness of Collaborative Learning Systems
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://pps-lab.com/people/anwarhithnawi/"><b>Anwar Hithnawi</b></a> (ETH Zurich)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                Collaborative, secure ML paradigms have emerged as a compelling alternative for sensitive applications in the last few years. These paradigms eliminate the need to pool data centrally for ML training and thus ensure data sovereignty and alleviate the risks associated with the large-scale collection of sensitive data. Although they provide many privacy benefits, these systemsamplify ML robustness issues by exposing the learning process to an active attacker that can be present throughout the entire training process. In this talk, I will give an overview of the security and robustness challenges of Collaborative Learning Systems and highlight why a definitive solution to robustness in these systems is challenging.
            </td>
        </tr>
    </table>
    <table id="Ce Zhang">
        <tr>
            <td class="date" rowspan="3">
                11:30am
            </td>
            <td class="title">
                TBA
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://ds3lab.inf.ethz.ch/members/ce-zhang.html"><b>Ce Zhang</b></a> (ETH Zurich)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                TBA
            </td>
        </tr>
    </table>
    <table id="Sai Praneeth Karimireddy">
        <tr>
            <td class="date" rowspan="3">
                12:00pm
            </td>
            <td class="title">
                Mechanisms which Incentivize Data Sharing in Collaborative Learning
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://spkreddy.org/"><b>Sai Praneeth Karimireddy</b></a> (University of California, Berkeley)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                In collaborative learning, under the expectation that other agents will share their data, rational agents may be tempted to engage in detrimental behavior such as free-riding where they contribute no data but still enjoy an improved model. In this work, we propose a framework to analyze the behavior of such rational data generators. We first show how a naive scheme leads to catastrophic levels of free-riding. Then, using ideas from contract theory, we show how to maximize the amount of data generated and provably prevent free-riding.
            </td>
        </tr>
    </table>
    <table>
        <tr>
            <td class="date" rowspan="2">
                12:30am
            </td>
            <td class="title-special">
                Lunch Break
            </td>
        </tr>
        <tr>
            <td class="abstract">
                <!-- Kept for Spacing -->
            </td>
        </tr>
    </table>
    <table id="Peter Richtarik">
        <tr>
            <td class="date" rowspan="3">
                1:30pm
            </td>
            <td class="title">
                On 5th Generation of Local Training Methods in Federated Learning
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://richtarik.org"><b>Peter Richtarik</b></a> (KAUST)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                Due to its communication-saving capabilities observed in practice, local training has been a key component of federated learning algorithms since the beginning of the field. However, much to the dismay of the theoreticians, the goal of confirming these practical benefits with satisfactory theory remained elusive. While our algorithmic understanding of local training already evolved through four generations---1) heuristic, 2) homogeneous, 3) sublinear and 4) linear---each advancing in one way or another over the previous one, the theoretical results belonging to this genre did not manage to provide communication complexity rates that would uncover any benefits coming from local training in the important heterogeneous data regime. In this talk, I will give a brief introduction to the 5th generation of local training methods, the first works of which were written in 2022. These methods enjoy strong theory, finally confirming that local training leads to theoretical communication acceleration.
            </td>
        </tr>
    </table>
    <table id="Sebastian U. Stich">
        <tr>
            <td class="date" rowspan="3">
                2:00pm
            </td>
            <td class="title">
                Heterogeneity-aware optimization
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://sstich.ch/"><b>Sebastian U. Stich</b></a> (CISPA)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                We consider instances of collaborative learning problems where training data is stored in a decentralized manner and distributed across multiple client devices (this covers both decentralized and federated learning settings). We characterize the impact of data heterogeneity on the convergence rate of standard training algorithms used in such collaborative learning environments.<br>
                We show how data-heterogeneity metrics can guide topology design for decentralized optimization, but also inspire bias-correction mechanisms to accelerated training. We will conclude by discussing limitations and open problems.
            </td>
        </tr>
    </table>
    <table id="Samuel Horvath">
        <tr>
            <td class="date" rowspan="3">
                2:30pm
            </td>
            <td class="title">
                Fair and Accurate Federated Learning under heterogeneous targets
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://sites.google.com/view/samuelhorvath"><b>Samuel Horvath</b></a> (MBZUAI)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                Federated Learning (FL) has been gaining significant traction across different ML tasks, ranging from vision to keyboard predictions. In large-scale deployments, client heterogeneity is a fact and constitutes a primary problem for fairness, training performance and accuracy. Although significant efforts have been made into tackling statistical data heterogeneity, the diversity in the processing capabilities and network bandwidth of clients, termed system heterogeneity, has remained largely unexplored. Current solutions either disregard a large portion of available devices or set a uniform limit on the model's capacity, restricted by the least capable participants.In this talk, we introduce FjORD taht alleviates the problem of client system heterogeneity by tailoring the model width to the client's capabilities.
            </td>
        </tr>
    </table>
    <table id="Martin Takáč">
        <tr>
            <td class="date" rowspan="3">
                3:00pm
            </td>
            <td class="title">
                FLECS: A Federated Learning Second-Order Framework via Compression and Sketching
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="http://mtakac.com"><b>Martin Takáč</b></a> (MBZUAI)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                Inspired by the recent work FedNL (Safaryan et al, FedNL: Making Newton-Type Methods Applicable to Federated Learning), we propose a new communication efficient second-order framework for Federated learning, namely FLECS. The proposed method reduces the high-memory requirements of FedNL by the usage of an L-SR1 type update for the Hessian approximation which is stored on the central server. A low dimensional `sketch' of the Hessian is all that is needed by each device to generate an update, so that memory costs as well as number of Hessian-vector products for the agent are low. Biased and unbiased compressions are utilized to make communication costs also low. Convergence guarantees for FLECS are provided in both the strongly convex, and nonconvex cases, and local linear convergence is also established under strong convexity. Numerical experiments confirm the practical benefits of this new FLECS algorithm.
            </td>
        </tr>
    </table>
    <table>
        <tr>
            <td class="date" rowspan="2">
                3:30pm
            </td>
            <td class="title-special">
                Coffee &amp; Tea Break
            </td>
        </tr>
        <tr>
            <td class="abstract">
                <!-- Kept for Spacing -->
            </td>
        </tr>
    </table>
    <table id="Eduard Gorbunov">
        <tr>
            <td class="date" rowspan="3">
                4:00pm
            </td>
            <td class="title">
                Variance Reduction is an Antidote to Byzantines: Better Rates, Weaker Assumptions and Communication Compression as a Cherry on the Top
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a href="https://eduardgorbunov.github.io/"><b>Eduard Gorbunov</b></a> (MBZUAI)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                Byzantine-robustness has been gaining a lot of attention due to the growth of the interest in collaborative and federated learning. However, many fruitful directions, such as the usage of variance reduction for achieving robustness and communication compression for reducing communication costs, remain weakly explored in the field. This work addresses this gap and proposes Byz-VR-MARINA - a new Byzantine-tolerant method with variance reduction and compression. A key message of our paper is that variance reduction is key to fighting Byzantine workers more effectively. At the same time, communication compression is a bonus that makes the process more communication efficient. We derive theoretical convergence guarantees for Byz-VR-MARINA outperforming previous state-of-the-art for general non-convex and Polyak-Lojasiewicz loss functions. Unlike the concurrent Byzantine-robust methods with variance reduction and/or compression, our complexity results are tight and do not rely on restrictive assumptions such as boundedness of the gradients or limited compression. Moreover, we provide the first analysis of a Byzantine-tolerant method supporting non-uniform sampling of stochastic gradients. Numerical experiments corroborate our theoretical findings.
            </td>
        </tr>
    </table>
    <table class="plenary">
        <tr>
            <td class="date" rowspan="2">
                4:30pm
            </td>
            <td class="title">
                Panel Discussion: Theoretical advances and challenges in collaborative learning
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Panel discussing theoretical questions in FL.
            </td>
        </tr>
    </table>
</body>
</html>

